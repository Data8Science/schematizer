# -*- coding: utf-8 -*-
import datetime
import functools
import logging
import thread
import time
import traceback

import MySQLdb
import sqlalchemy.exc

from yelp_conn import engine
from yelp_conn.mysqldb import LOCK_WAIT_TIMEOUT
from yelp_conn.mysqldb import LOCK_DEADLOCK
from yelp_conn.mysqldb import SERVER_GONE_ERROR
from yelp_conn.mysqldb import OPERATIONAL_ERRORS

KEEP_ALIVE_CONN_RETRIES = 4
HEATBEAT_WAIT_TIME_SECS = 0.250

log = logging.getLogger(__name__)


class dbcursor(object):
    """Block to run with a cursor, ensuring its closed afterwards.

      with dbcursor(connection) as cursor:
        # do something with cursor here
    """

    def __init__(self, conn):
        self.conn = conn

    def __enter__(self):
        self.cursor = self.conn.cursor()
        return self.cursor

    def __exit__(self, type, value, traceback):
        self.cursor.close()
        return False


def wait_for_replication_sql_thread(master_conn, slave_conn, start_thread=False):
    master_cursor = master_conn.cursor()
    master_cursor.execute('SHOW MASTER STATUS')
    log_file, log_pos, _, _ = master_cursor.fetchone()

    log.debug("Repl Waiting for %s %d", log_file, log_pos)

    # Re-start replication, force it to catch up
    slave_cursor = slave_conn.cursor()
    if start_thread:
        slave_cursor.execute('START SLAVE SQL_THREAD')

    slave_cursor.execute("SELECT MASTER_POS_WAIT('%s', %d)" % (log_file, log_pos))
    result = slave_cursor.fetchone()[0]
    log.debug("Replication MASTER_POS_WAIT response: %r", result)

    return result


def wait_for_heartbeat(master_conn, slave_conn, max_replication_delay):
    """Wait for replication to catch up, as defined by the replication hearbeat

    We have a process that causes a heartbeat to go through to all our
    replicas. By waiting for a certain heartbeat to show up on the specified
    slave_conn, we can be assured that all writes prior to that heartbeat are
    available.

    Keep in mind that this function could delay up to max_replication_delay
    before giving up, so use with caution. Also, the minimum amount of time we must
    wait is 1 second, as we can't assume the master's current heartbeat is AFTER
    the write we care about.
    """
    log.info("Waiting for heartbeat")
    master_cursor, slave_cursor = master_conn.cursor(), slave_conn.cursor()

    master_cursor.execute("SELECT timestamp FROM yelp_heartbeat.replication_heartbeat")
    master_time = master_cursor.fetchone()[0]
    master_conn.rollback()
    master_cursor.close()

    # We are going to look for the next heartbeat, because we dont' know how long ago the master got heartbeated.
    # This means, for this function to safely work it's probably going to end up taking at least 1 second, best case scenario
    heartbeat_to_wait = master_time + datetime.timedelta(seconds=1)

    local_start_time = time.time()
    ndx = 0
    while time.time() - local_start_time <= max_replication_delay:
        slave_cursor.execute("SELECT timestamp FROM yelp_heartbeat.replication_heartbeat")
        slave_time = slave_cursor.fetchone()[0]
        slave_conn.rollback()

        if slave_time >= heartbeat_to_wait:
            log.info("Saw heartbeat after %.2f seconds", time.time() - local_start_time)
            break

        log.debug("Waiting on replication delay (%d)", ndx)
        time.sleep(HEATBEAT_WAIT_TIME_SECS)
        ndx += 1
    else:
        raise Exception("Waited %.2f seconds for replication to catchup." % (time.time()-local_start_time))

    slave_cursor.close()


class MaxOperationalErrorsReached(Exception): pass


def retry_on_db_op_error_with_delay(
        retries=3,
        op_err_codes=None,
        delay=1.0,
        reraise=True,
        restrict_to_thread=False):
    """Decorator that does retries delay seconds apart in case of one of
    the specified mysqldb OperationalError.

    The decorated method will be called, and if one of the specified
    `op_err_codes` occur, the method will be called the specified number
    of times, 1s apart after a connection rollback.

    The decorated method must be idempotent, as it can get called multiple times.

    After the given number the retries, the OperationalError exception is rethrown.

    :param retries: number of retries to attempt.
    :param op_error_codes: a tuple of OperationalError's on which to retry.
    :param delay: number of seconds to sleep between retries.
    :param reraise: whether to re-raise the OperationalError after giving up.
                    When false, a MaxOperationalErrorsReached error is raised
                    instead. This can prevent jobs from potentially being
                    re-queued by gearman after repeatedly failing.
    :param restrict_to_thread: restrict rollback to engine_managers that share the same
            thread id as the current thread. Commonly used when the connection_set is
            configured not to share engine_managers - `bypass=True`. Defaults to false
            because the majority of the existing uses of this decorator are in
            single-threaded applications or batch jobs.
    """
    if op_err_codes is None:
        op_err_codes = (
            LOCK_WAIT_TIMEOUT,
            LOCK_DEADLOCK,
            SERVER_GONE_ERROR,
        )
    def decorator(method):
        @functools.wraps(method)
        def wrapper(*args, **kwargs):
            for _ in xrange(retries):
                try:
                    return method(*args, **kwargs)
                except OPERATIONAL_ERRORS as e:
                    given_self = None if not args else args[0]
                    logger = getattr(given_self, 'log', None) or getattr(given_self, 'logger', None) or log

                    if isinstance(e, MySQLdb.OperationalError):
                        err_code, message = e.args
                    elif isinstance(e, sqlalchemy.exc.OperationalError):
                        err_code, message = e.orig.args

                    logger.warn('OperationalError %r %r' % (err_code, message))
                    if err_code in op_err_codes:
                        # performs a rollback if our connection is still alive, otherwise
                        # this will dispose the dead connection and grab a new one for us.
                        if restrict_to_thread:
                            # issues a rollback on engine managers which were created
                            # by the current thread of execution
                            for engine_manager in engine.all_engine_managers():
                                if engine_manager.thread_id == thread.get_ident():
                                    engine_manager.rollback()
                        else:
                            # issues a rollback on all engine managers
                            engine.rollback()

                        time.sleep(delay)
                    else:
                        raise

            # gearman will automatically retry our job if it sees an op error, we don't want that.
            # note we only reach this block if we had an OperationalError above on our last retry.
            if not reraise:
                raise MaxOperationalErrorsReached(traceback.format_exc())
            else:
                raise

        return wrapper
    return decorator


class OperationError(Exception): pass


def wait_for_named_lock(conn, lock_name, max_timeout=0, log=None):
    """
    This waits for the lock specified by @lock_name for up to @max_timeout
    seconds. Set @max_timeout to 0 or None to wait forever.

    Returns:
        True if the lock was acquired, False otherwise
    """

    if log:
        log.info('Acquiring MySQL named lock %r' % (lock_name,))

    def acquire_lock(timeout):
        cursor = conn.cursor()
        status = cursor.execute("SELECT GET_LOCK(%s, %s)", (lock_name, timeout))
        cursor.close()

        # According to MySQL documentation:
        #   NULL [is returned] if an error occurred (such as running out of
        #   memory or the thread was killed with mysqladmin kill)
        # See http://dev.mysql.com/doc/refman/5.0/en/miscellaneous-functions.html#function_get-lock
        if status is None:
            raise OperationError

        status = bool(status)
        if status and log:
            log.info('Lock %r acquired' % (lock_name,))
        return status

    if max_timeout:
        return acquire_lock(max_timeout)

    # Otherwise the user wants to wait forever, so we call acquire lock in
    # a loop
    start_time = time.time()
    while True:
        result = acquire_lock(3600)
        if result:
            return result
        elif log:
            log.warning("Still waiting for lock %s after %d secs", lock_name, time.time() - start_time)

# -*- coding: utf-8 -*-
"""Functions for bulk-loading data into the MySQL database."""
from calendar import timegm
from datetime import datetime
import os
import tempfile
from multiprocessing import Process
import shutil


def load_data_infile(cursor,
                     filename,
                     tbl_name,
                     field_terminate=None,
                     field_enclose=None,
                     field_escaped=None,
                     line_start=None,
                     line_terminate=None,
                     low_priority=False,
                     concurrent=False,
                     local=True,
                     replace=False,
                     ignore=False,
                     ignore_lines=0,
                     column_names=None):
    """Run a LOAD DATA INFILE statement to bulk-load data into
    a table.

    To prepare an infile, you can use write_row_to_infile() which will match
    MySQLs default infile format.

    Args:
    cursor -- the db cursor
    filename -- the name of the file to load from
    tbl_name -- the name of the table to dump data to
    field_terminate -- string that terminates field. MySQL default '\t'
    field_enclose -- char that encloses a field. MySQL default ''
    field_escaped -- char that escapes a field. MySQL default '\\'
    line_start -- string at the start of all lines. MySQL default ''
    line_terminate -- string that terminates line. MySQL default '\n'
    low_priority -- if this is true, wait until no one is reading from this table
    concurrent -- allow other threads to read while writing data (a little slower)
    local -- load from a file locally, rather than one on the database server
    replace -- if a row you're inserting would cause a duplicate key error, delete
        the conflicting row
    ignore -- if a row you're inserting would cause a duplicate key error, delete
        the conflicting row, don't insert it (if neither replace or ignore is set,
        it works like ignore=True when local is True, and otherwise raises an error)
    ignore_lines -- skip this many lines at the beginning of the file
    column_names -- a list of column names to load data into

    Returns:
    Number of rows affected
    """
    if low_priority and concurrent:
        raise ValueError("Can't set both LOW PRIORITY and CONCURRENT")

    if replace and ignore:
        raise ValueError("Can't set both REPLACE and IGNORE")

    field = None
    if field_terminate or field_enclose or field_escaped:
        field = "FIELDS "
        if field_terminate:
            field += "TERMINATED BY %s " % field_terminate
        if field_enclose:
            field += "ENCLOSED BY %s " % field_enclose
        if field_escaped:
            field += "ESCAPED BY %s " % field_escaped

    line = None
    if line_start or line_terminate:
        line = "LINES "
        if line_terminate:
            line += "TERMINATED BY %s " % line_terminate
        if line_start:
            line += "STARTING BY %s " % line_start

    sql = (
        "LOAD DATA {low_priority}{concurrent}{local}INFILE '{filename}' "
        "{replace}{ignore}INTO TABLE `{table_name}` CHARACTER SET utf8 "
        "{field}{line}{ignore_lines}{column_names}".format(
            low_priority='LOW PRIORITY ' if low_priority else '',
            concurrent='CONCURRENT ' if concurrent else '',
            local='LOCAL ' if local else '',
            filename=filename,
            replace='REPLACE ' if replace else '',
            ignore='IGNORE ' if ignore else '',
            table_name=tbl_name,
            field=field if field else '',
            line=line if line else '',
            ignore_lines=('IGNORE %i LINES' % ignore_lines) if ignore_lines else '',
            column_names=('(%s) ' % ','.join(column_names)) if column_names else ''
        )
    )

    cursor.execute(sql)
    return cursor.rowcount


def _esc_str_for_infile(s):
    """Escape the str s for a MySQL infile. Helper for write_row_to_infile()"""
    s = s.replace('\\', '\\\\')
    s = s.replace('\n', '\\n')
    s = s.replace('\t', '\\t')
    s = s.replace('\0', '\\0')
    # can also escape \r, \b, and \x16 (as \Z), but there's no need
    return s

def _esc_for_infile(x):
    """Escape x for a MySQL infile. Helper for write_row_to_infile()"""
    if x is None:
        return '\\N'
    elif isinstance(x, bool):
        return '1' if x else '0'
    # don't worry about inf, etc.
    elif isinstance(x, (int, long, float)):
        return repr(x)
    elif isinstance(x, str):
        return _esc_str_for_infile(x)
    elif isinstance(x, unicode):
        return _esc_str_for_infile(x.encode('utf8'))
    elif isinstance(x, datetime):
        return repr(timegm(x.timetuple()))
    else:
        raise TypeError("Value has unsupported type: %r" % (x,))

def write_row_to_infile(out, row):
    """Write the given sequence of values (row) out to the given file (out).

    For example, input like: [123.45, 'hello', None, -43]

    writes out a tab-delimited line like this (with a \n at the end):
    123.45    hello    \N    43

    Unicode values are automatically escaped to UTF-8.

    We only support this one format for escaping because it's simple
    and compact, and supports all values, including NULL.
    """
    line = '\t'.join(map(_esc_for_infile, row)) + '\n'
    out.write(line)

def load_data(cursor, table_name, fieldnames, dicts, temp_dir=None):
    """Bulk loads the data in dicts into table_name.

    Replaces :func:`yelp_conn.mysql.misc.load_data`, which is incompatible with
    conventions in this module. This is analogous to load_data_infile
    except that it operates on dicts directly and makes the files for you.

    A named pipe is used instead of a file to avoid writing the values to disk.

    :param cursor: a dbi cursor
    :param table_name: name of the destination table
    :param fieldnames: list of field names
    :param dicts: list of dicts which will be loaded into a table
    :param temp_dir: temporary directory to use
    """

    local_temp_dir = None
    filename = None

    try:
        # Make a folder in the temp dir.  Guaranteed to be unique.
        local_temp_dir = tempfile.mkdtemp(dir=temp_dir)
        filename = os.path.join(local_temp_dir, 'tmp-bulk-loader')
        os.mkfifo(filename)

        def write_rows():
            with open(filename, 'wb') as infile:
                for row in dicts:
                    write_row_to_infile(infile, [row.get(field, None) for field in fieldnames])

        # A new process is needed as the named pipe will block when only one
        # process has opened the pipe
        write_rows_proc = Process(target=write_rows)
        write_rows_proc.start()

        load_data_infile(cursor, filename, table_name,
                         local=True,
                         replace=True,
                         column_names=fieldnames)

    finally:
        # Cleanup
        if write_rows_proc:
            write_rows_proc.join()
        if local_temp_dir:
            shutil.rmtree(local_temp_dir)

# -*- coding: utf-8 -*-
"""Implements special behavior on top of SQLAlchemy engines.

Engines are the interface sqlalchemy uses to interact with real database connections.
Or at least it's one part of the stack.

Engines are composed of a pool, which are composed of real DBAPI connections.
An engine exists forever, or at least possibly longer than it's underlying pool or connection
as certain events can cause us to choose to reconnect.
"""
from __future__ import absolute_import

import datetime
import logging
import sys
import thread
import time
import weakref

import MySQLdb
import sqlalchemy.event
import sqlalchemy.exc
from staticconf import schema

from yelp_conn import _import
from yelp_conn import config
from yelp_conn import dialect
from yelp_conn.sqla_backports import QueuePool
from yelp_conn.topology import ConnectionDef


log = logging.getLogger(__name__)

# We want to keep track of all our engines so we can
# do fancy stuff like drop them in mass. To accomplish this without
# leaking memory all over the place we're just going to store weakrefs
# which add some amount of complexity.
_engine_mngr_set = weakref.WeakKeyDictionary()

# We also keep track of the defacto engine for a given connection definition.
# There could potentially be more engines to a given connection def due to
# threads and the like (bypass), but generally we want to keep the same engine
# around. We want these to be persistent so no weakref.
_engine_mngr_registry = {}


class InvalidThreadError(Exception):
    """Indicates a thread is accessing a engine manager it shouldn't"""
    pass


class MysqlDialectUrl(object):
    """
    This class is used as a `sqlalchemy.engine.url.Url` class passed to
    sqlalchemy.create_engine() to create an engine that uses our custom
    MySQLDialect. Normally these url classes are responsible for parsing
    connection details but our connection pool code takes care of that
    seperately, so we just stub the expected methods.

    See: http://docs.sqlalchemy.org/en/latest/core/engines.html#sqlalchemy.engine.url.URL
    """

    # Required by sqlalchemy.dialect.DefaultDialect.create_connect_args()
    query = {}

    @classmethod
    def get_dialect(cls):
        """Return the dialect to use for the engine."""
        return dialect.MysqlFastConnectDialect

    @classmethod
    def translate_connect_args(cls, **kwargs):
        # Required by sqlalchemy.dialect.DefaultDialect.create_connect_args()
        return {}


class EngineManagerConfig(schema.Schema):
    """A configuration object which is the `new` namesapce for engine
    configuration.
    """

    namespace = config.namespace

    pool_recycle_time = schema.int(
        config_key='engine.pool.recycle',
        default=60*60*2,
        help="How often the connections will get refreshed (seconds).")


class EngineManagerDbConfig(schema.Schema):
    """A configuration object which is the `legacy` namespace for some
    engine configuration.

    These configuration values should be migrated to either
    :class:`EngineManagerConfig`, or split out so that they are not necessary
    in :class:`EngineManager`.
    """

    namespace = config.namespace

    config_path = 'db'

    replication_delay_check_interval = schema.float(default=5)

    max_replication_delay = schema.int(default=120)

    max_connection_time = schema.float(
            default=600.0,
            help="How long to persist db connections before having them reconnect.")

    replication_delay_fudge = schema.float(default=3)

    enable_connection_refresh = schema.bool(
            default=True,
            help="Expire connections after max_connection_time below and reconnect")

    enable_replication_delay_reconnect = schema.bool(
            default=True,
            help="Have our database connection pools periodically check of the replica "
                 "is too far behind, and then reconnect.")

    test_strict_thread_db_access = schema.bool(default=False)

    db_pool_listeners = schema.list(default=[])


class EngineManager(object):

    def __init__(
            self,
            connection_def,
            creator=None,
            params=None,
            config=None,
            db_config=None):
        self.config = config or EngineManagerConfig()
        self.db_config = db_config or EngineManagerDbConfig()
        self._connection_def = connection_def
        self._creator = creator
        self._params = params or {}
        if self._creator is None and not self._params:
            raise ValueError("Either params or creator must be set")

        self._engine = None
        self.open_time = None
        self.last_check_time = None
        self.last_delay_heartbeat_time = None
        self.thread_id = thread.get_ident()

        _engine_mngr_set[self] = True

    def __repr__(self):
        return "EngineManager: %s" % (self._connection_def,)

    def set_open_time(self):
        self.open_time = time.time()

    @property
    def is_slave(self):
        return self._connection_def.slave

    @property
    def engine(self):
        if self._engine is None:
            self._engine = self._create_engine()

        return self._engine

    def pool_is_open(self):
        """Indicates the pool has open connections.

        This is useful for times when we need to manually manage all our
        connection but we only care if the connection is open (things like
        rollback).

        overflow() is initialized to -pool.maxsize.  When added to pool.size(),
        it indicates the number of connections managed by the pool.
        """
        return (self.engine.pool.size() + self.engine.pool.overflow()) > 0

    def _create_engine(self):
        if self._connection_def.database == ConnectionDef.POSTGRES:
            new_engine = self._build_postgres_engine()
        else:
            new_engine = self._build_mysql_engine()

        # We need to make a few adjustments to our pool now that it's built.
        # These are all things that the create_engine function doesn't give us hooks for.
        sqlalchemy.event.listen(new_engine.pool, 'connect', EngineManagerPoolListener(self).connect)

        for listener in self.db_config.db_pool_listeners:
            try:
                listener_instance  = _import.import_module_class(listener)()
                sqlalchemy.event.listen(new_engine.pool, 'connect', listener_instance.on_connect)
                sqlalchemy.event.listen(new_engine.pool, 'checkout', listener_instance.on_checkout)
            except Exception:
                log.exception('Failed to instanciate db pool listener %s' % listener)

        return new_engine

    def _build_mysql_engine(self):
        new_pool = QueuePool(self._creator,
            recycle=self.config.pool_recycle_time,
            echo=False,
            use_threadlocal=True,
            reset_on_return=False)

        return sqlalchemy.create_engine(
            MysqlDialectUrl,
            pool=new_pool,
            paramstyle='pyformat',
            connection_params=self._params)

    def _build_postgres_engine(self):
        """This builds an engine for postgres using the stock sqlalchemy adapter.

        The Yelp connection adapters, many of which are msyql-specific are
        intentionally skipped.
        """
        conn = self._creator.get_conn_entry()
        conn_params = conn.connection_params
        url = 'postgresql+psycopg2://{user}:{passwd}@{host}:{port}/{db}'.format(**conn_params)
        return sqlalchemy.create_engine(url)

    @property
    def connection(self):
        return self.engine.pool.connect()

    def dispose(self):
        self.engine.dispose()
        self.open_time = None

    def rollback(self):
        if self.pool_is_open():
            self._rollback()

    def _rollback(self):
        try:
            self.connection.rollback()
        except Exception as e:
            # If we fail during a rollback it isn't really that critical since
            # we can just re-establish the connection and carry on. But we're
            # going to log it anyway incase it's happening more than it should.
            # TODO: Mike Bayer says we do not need this because SQLAlchemy has
            # built in support for this
            log.error("Failed to rollback on %r: %r", self, e)
            self.dispose()

    def refresh(self):
        if self.pool_is_open():
            self._rollback()

            log.debug("Refreshing %r", self)
            try:
                self._refresh_connection()
            except (MySQLdb.MySQLError,
                    MySQLdb.OperationalError,
                    sqlalchemy.exc.DisconnectionError) as e:
                # Queries on a stale conection can raise a variety of errors; we usually see
                # OperationalError(2006, 'MySQL server has gone away')
                # But whatever, we were done with this connection anyway, we'll just get a new one.
                log.error("Failed to rollback on %r: %r", self, e)
                self.dispose()

    @property
    def max_replication_delay(self):
        """Return the potential replication delay we might be seeing.

        This is different the actual replication delay because we never have
        instanteous information.  We only check for replication delay every
        replication_delay_check_interval, so replication could have stopped
        right after the last time we checked.

        Also, there is some inherent inaccuracies that is represented by
        replication_delay_fudge
        """
        if not self.is_slave:
            return None

        if self.open_time is None:
            log.error("No open time. Out of band max_replication_delay check?")
            return None

        # If we've never checked replication before, and the connection has
        # been open for replication_delay_check_interval, check now.
        if (self.last_delay_heartbeat_time is None and
            time.time() - self.open_time >
                self.db_config.replication_delay_check_interval):
            self._check_replication()

        # Is our normal replication checking interval up ?
        elif (self.last_delay_heartbeat_time and
              time.time() - self.last_check_time >
                self.db_config.replication_delay_check_interval):
            self._check_replication()

        # Note that this calculation has the last check time involved, since
        # last_delay_heartbeat_time is the last timestamp we saw, but we may
        # have checked up to replication_delay_check_interval ago. So if we
        # were within 25 seconds of max replication delay, and we checked 25
        # seconds ago, we'll drop the connection just in case.
        if self.last_delay_heartbeat_time is not None:
            fudged_ts = time.time() + self.db_config.replication_delay_fudge
            actual_delay = fudged_ts - self.last_delay_heartbeat_time
            return float(actual_delay)
        else:
            return None

    def _refresh_connection(self):
        if not self.db_config.enable_connection_refresh:
            return

        if self.open_time is None:
            log.error("No open time. Out of band _refresh_connection check?")
            return None

        log.debug("Checking/Refreshing connection %r (%.2f)", self, (time.time() - self.open_time))
        if time.time() - self.open_time >= self.db_config.max_connection_time:
            log.info("Connection %r has expired, dropping", self)
            self.dispose()
            return

        if not self.db_config.enable_replication_delay_reconnect:
            return

        # Note that max_replication_delay will be None if the question doesn't make sense
        max_replication_delay = self.max_replication_delay
        if (max_replication_delay is not None and
                max_replication_delay >= self.db_config.max_replication_delay):
            log.warning("Connection %s is too delayed, dropping", self.connection.name)
            self.dispose()
            return

    def _check_replication(self):
        """Check the current open connection for replication delay

        Note that this should never be called during the middle of using this
        connection, as transaction isolation might screw things up.
        """
        assert self.is_slave

        connection = self.connection
        cursor = connection.cursor()
        self.last_check_time = None
        self.last_delay_heartbeat_time = None

        # We're doing straight SQL here because we don't want to create a dependency on logic (and EZA)
        try:
            cursor.execute("SELECT timestamp FROM yelp_heartbeat.replication_heartbeat")
        except MySQLdb.MySQLError as e:
            # If we fail here it means either the connection went away or the table doesn't support heartbeat.
            # Worth complaining about, but maybe not worth failing right away.
            errno, errmsg = e
            log.error("Failed trying to check replication on %s: %r", connection.description, errmsg)
            return

        result = cursor.fetchall()
        cursor.close()
        if len(result) == 0:
            log.error('No rows in yelp_heartbeat.replication_heartbeat for %s', connection.description)
            return

        # TODO This method is copied from util.timeutil. Moving timeutil to yelp_lib will happen at some point
        # in the future. Remove this duplicate method at that time.
        def to_timestamp(datetime_val):
            if datetime_val is None:
                return None

            # If we don't have full datetime granularity, translate
            if isinstance(datetime_val, datetime.datetime):
                datetime_val_date = datetime_val.date()
            else:
                datetime_val_date = datetime_val

            if datetime_val_date >= datetime.date.max:
                return sys.maxint

            return int(time.mktime(datetime_val.timetuple()))


        self.last_delay_heartbeat_time = to_timestamp(result[0][0])
        self.last_check_time = time.time()

        delay_in_seconds = time.time() - self.last_delay_heartbeat_time
        log.debug("%s saw replication delay of %d", connection.description, delay_in_seconds)


class EngineManagerPoolListener(object):
    """This is a SQLAlchemy Connection Pool listener that the EngineManager uses
     to track the operation of it's underlying pool.

    This admittedly makes for a fairly complex relationship between
    EngineManager->Engine->Pool->EngineManagerPoolListener, but what about
    SQLAlchemy has ever not been complex.
    """
    def __init__(self, engine_manager):
        self._manager = weakref.ref(engine_manager)

    def connect(self, dbapi_con, con_record):
        if self._manager and self._manager():
            self._manager().set_open_time()


class DisconnectedPoolListener(object):
    """This event listener watches for new pool checkouts (connections to
    MySQL servers are pooled for performance) and pings them on checkout.
    This will allow us to catch a down / stale connection before trying to
    do real work and gets us a fresh connection.

    See http://y/sqlalchemy-disconnect-handling for more information.
    """

    def on_connect(self, dbapi_con, con_record):
        # We only operate on checkouts
        pass

    def on_checkout(self, dbapi_con, con_record, con_proxy):
        cursor = dbapi_con.cursor()
        try:
            # Ping the connection
            cursor.execute("SELECT 1")
        except (MySQLdb.MySQLError, MySQLdb.OperationalError):
            # This exception is handled further up and discards the connection
            raise sqlalchemy.exc.DisconnectionError()
        cursor.close()


def create_engine_manager(connection_def, creator):
    return EngineManager(connection_def, creator)

def get_or_create_engine_manager(connection_def, creator):
    """Returns the single engine manager for a specific connection def. This is
    used to limit the number of connections per process.
    """
    if connection_def not in _engine_mngr_registry:
        _engine_mngr_registry[connection_def] = create_engine_manager(connection_def, creator)

    return _engine_mngr_registry[connection_def]

def all_engine_managers():
    # Use keys to try to prevent gc while iterating, see ticket #59733
    return list(_engine_mngr_set.keys())

def all_engines():
    return [manager.engine for manager in all_engine_managers()]

def refresh():
    """Refresh all available connections.
    """
    for mngr in all_engine_managers():
        mngr.refresh()

def reset():
    """Reset (close) all available connection
    """
    for mngr in all_engine_managers():
        mngr.dispose()

def reset_and_uncache():
    """Reset (close) all available connections and forget about them in the
    cache.
    """
    global _engine_mngr_registry
    reset()
    _engine_mngr_registry = {}

def rollback():
    """Rollback on all available connections
    """
    for mngr in all_engine_managers():
        mngr.rollback()

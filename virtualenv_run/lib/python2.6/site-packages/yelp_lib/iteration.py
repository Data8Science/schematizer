"""
Utility functions for manipulating iterables.
"""

import itertools


def ilen(iterable):
    """Returns the length of a sequence (this also exhausts the sequence)"""
    k = 0
    for _ in iterable:
        k += 1
    return k


def count_items(iterable):
    """
    Count items in a sequence without requiring hashability/dictionaries.

    [X,Y,X,Y,Y,Y,Y,Y] -> [(X, 2), (Y, 6)]

    ~(n^2lgn) comparisons, 2n storage?
    """
    # by default, itertools.groupby groups consecutive sorted elements by ==,
    # like the unix `uniq` command:
    x = itertools.groupby(sorted(iterable))
    def _counts():
        for k, items in x:
            yield (k, ilen(items))

    return tuple(_counts())


def count_hashable_items(iterable):
    """
    Count items in a sequence that we CAN hash, returning
    a dictionary mapping item to the number of times it appears.
    """
    results = {}
    for i in iterable:
        results.setdefault(i, 0)
        results[i] += 1

    return results


def winnow(function, iterable):
    """Like filter, except we return the rejects too.

    Returns (passed, failed), where passed is the list of items where
    function(item) was true, and failed are the other items.
    """
    passed = []
    failed = []
    for item in iterable:
        if function(item):
            passed.append(item)
        else:
            failed.append(item)

    return passed, failed


def first(iterator, matching_function=None):
    """Get the first element from an iterable or None"""
    if not iterator:
        return None
    for i in iterator:
        if matching_function is None or matching_function(i):
            return i


def uniq(seq, key=lambda x: x, limit=None):
    """
    Given a sequence, yield the distinct items in the
    sequence, in the order that they first appear.

    Args:
    seq -- the sequence
    key -- A function that takes an item in the sequence, and
        returns a key that we want to be distinct.
    limit -- a maximum number of items to return
    """
    already_seen = set()

    for item in seq:
        if limit is not None and len(already_seen) >= limit:
            return

        k = key(item)
        if k not in already_seen:
            yield item
            already_seen.add(k)


def unix_uniq(seq):
    """
    Given a sequence, compare successive items and return any that
    don't equal the previous key. Useful when your sequence is
    composed of dictionaries or other unhashable items
    """
    last = None
    first = True

    for item in seq:
        if first or item != last:
            yield item
        last = item
        first = False


def window(iterator, size=5):
    """Read from an iterator and return overlapping windows of size `size`.

    Args:
        iterator: Existing iterator that we will window.

    Kwargs:
        size: window size.

    Yields:
        In-order overlapping lists of size `size` from `iterator`.
    """
    buf = []
    for line in iterator:
        buf.append(line)
        if len(buf) > size:
            buf.pop(0)
        if len(buf) == size:
            yield buf


class RunLengthEncodingError(Exception):
    pass


class RunLengthDecodingError(Exception):
    pass


def run_length_encode(iterable):
    """
    Performs a simple run length encoding on an iterable. Runs of repeated
    elements are replaced by the tuple (element, length).

    To avoid decoding ambiguities, and to keep the implementation simple,
    elements cannot be lists or tuples.

    Assuming all elements are JSON-encodable, the encoded data is designed to
    be JSON-encodable.

    Args:
        iterable - sequence to encode. (cannot contain lists or tuples)

    Yields:
        either individual elements, or the pair (element, length)

    Raises:
        RunLengthEncodingError if any element is a list or tuple.
    """
    for key, group in itertools.groupby(iterable):
        if isinstance(key, tuple) or isinstance(key, list):
            raise RunLengthEncodingError(
                "This implementation of run length encoding is not well-defined "
                "for iterables of tuples or lists.")

        length = ilen(group)
        if length == 1:
            yield key
        else:
            yield (key, length)


def run_length_decode(iterable):
    """
    Decodes an iterable encoded by run_length_encode(). Expects either single
    elements or pairs of (element, length).

    Args:
        iterable - sequence encoded by run_length_encode()

    Yields:
        individual elements

    Raises:
        RunLengthDecodingError
    """
    for item in iterable:
        # Check both tuple and list, since JSON-encoding converts tuples to lists.
        if isinstance(item, tuple) or isinstance(item, list):
            if len(item) != 2:
                raise RunLengthDecodingError("Got unexpected sequence %s of length %i." % (item, len(item)))

            val, length = item
            for _ in xrange(length):
                yield val
        else:
            yield item


class SegmentProcessor(object):
    """ Push individual items via the push() method.  When the number of items pushed reaches "size",
        process_fn() is called with the current queue of items.  The queue is then emptied.

        This can be useful for segmented batch processing of objects.  For instance, queuing up a list of
        database rows that can that all be inserted en-masse.

        Can also be used as context.  When the context is exited, any pending items are forcibly processed.
        This is probably how you want to use SegmentProcessor. Otherwise you need to call flush() at the
        end of your function to ensure that you process any outstanding queue items.

        e.g.
        with SegmentedProcessor(100, self.insert_rows) as processor:
            processor.push(item1)
            processor.push(item2)
            processor.push(item...)


    """
    def __init__(self, size, process_fn):
        """ Establish a queue that of 'size' items. When the size is reached, process_fn() is called with the contents of the queue
        """
        self.size = size
        self.queue = []
        self.process_fn = process_fn

    def push(self, item):
        """ Push an item onto the queue.  If the queue 'size' limit is reached, the process_fn() will be called in-line.
        """
        self.queue.append(item)
        if len(self.queue) >= self.size:
            self.flush()

    def __enter__(self):
        """ For use as a context
        """
        return self

    def flush(self):
        """ Forcibly process any items in the queue
        """
        if self.queue:
            self.process_fn(self.queue)
            self.queue = []

    def __exit__(self, type, value, traceback):
        """ For use as a context.  If no exception was raised, process all remaining items of the queue
        """
        if value is None:
            self.flush()
        return False

"""
This module contains utilities for JSON encoding.

Originally (when it was util/json.py in yelp-main), it was a compatibility module
for switching between different JSON encoders, and it was the preferred way to
read and write JSON at Yelp. As of #35057, this is no longer its purpose. The
preferred way to read and write JSON is to import and use json directly.
"""
from __future__ import absolute_import

import copy
import datetime
import decimal
import functools
import re
try:
    import simplejson as json
except ImportError:
    import json

import yelp_lib._import


ReadException = ValueError
DecodeError = ValueError
EncodeError = ValueError

### utilities for dealing with non-UTF8 bytestrings ###

def latin1_decode_item(encodable_item):
    """Recursive latin-1 decoder to be used with writing Python dicts out to JSON
    latin1 decodes any byte strings that can't be UTF-8 decoded into unicode

    This latin-1 decodes strings returns a path of latin-1'ed keys/values in case
    you want to restore the bytes
    If you see an integer in the path, that is a list index.

    e.g.
    ('extra', 'hi', 2, 4, 'longer', 'dictionary', 'paths')
    """
    assert isinstance(encodable_item, (list, tuple, dict, basestring))
    rewritten_keys = []
    rewritten_values = []

    def _decode_to_unicode(value):
        # Skip the cases where someone has used an int or the like as a key
        if not isinstance(value, basestring):
            return value, False
        # Don't double-decode existing unicode objects: this will attempt to
        # encode them back to str using the ascii codec, then re-decode them
        # which causes a UnicodeEncodeError if the object contains non-ASCII
        # codepoints.
        if isinstance(value, unicode):
            return value, False
        try:
            value.decode('utf-8')
            return value, False
        except UnicodeDecodeError:
            return value.decode('latin-1'), True

    def _recursively_encode_item(in_item, in_path):
        out_item = None

        # Expand dicts, lists, and tuples... and try to replace their values in place
        if isinstance(in_item, dict):
            out_item = {}
            for key, value in in_item.items():
                new_key, was_rewritten = _decode_to_unicode(key)
                if was_rewritten:
                    rewritten_keys.append(in_path + [new_key])

                out_item[new_key] = _recursively_encode_item(value, in_path + [new_key])

        elif isinstance(in_item, (list, tuple)):
            out_item = list(in_item)
            for idx, value in enumerate(out_item):
                out_item[idx] = _recursively_encode_item(value, in_path + [idx])

            if isinstance(in_item, tuple):
                out_item = tuple(out_item)

        elif isinstance(in_item, basestring):
            out_item, was_rewritten = _decode_to_unicode(in_item)
            if was_rewritten:
                rewritten_values.append(in_path)

        else:
            out_item = in_item
        return out_item

    # Go ahead and latin1 encode our item
    output_item = _recursively_encode_item(encodable_item, [])

    # Return our handiwork
    static_key_path = tuple(map(tuple, rewritten_keys))
    static_value_path = tuple(map(tuple, rewritten_values))
    return output_item, static_key_path, static_value_path

def latin1_encode_item(decodable_item, latin_key_paths, latin_value_paths):
    """Returns an inverted latin1_decoded_item"""
    output_item = copy.copy(decodable_item)
    for value_path in latin_value_paths:
        # Traverse to the last item in th epath
        current_item = output_item
        for key in value_path[:-1]:
            current_item = current_item[key]

        # And rewrite the last value in place
        last_key = value_path[-1]
        assert isinstance(current_item[last_key], unicode)
        current_item[last_key] = current_item[last_key].encode('latin-1')

    # Start rewriting all our keys... everything in here should be a dict
    for key_path in latin_key_paths:
        last_key = key_path[-1]
        assert isinstance(last_key, unicode)

        current_item = output_item
        for key in key_path[:-1]:
            current_item = current_item[key]

        assert isinstance(current_item, dict)
        new_key = last_key.encode('latin-1')
        current_item[new_key] = copy.copy(current_item[last_key])
        del current_item[last_key]

    return output_item

# An extra key that is put in a dictionary that contains paths to strings
# that were originally latin1 so loads_with_bytes can re-encode them and return
# the dictionary exactly as it was pre-serialization
LATIN1_ENCODING_COOKIE = '__latin1_encoded__'
def dumps_with_bytes(serializable_dict, default=None):
    """
    JSON doesn't have a native bytestring type so json will automatically
    try to convert all strings to unicode strings using UTF-8. latin1 encoded
    strings will cause this to fail. This wrapper handles that cases and decodes
    all latin1 strings.

    NOTE: A more powerful and flexible approach to this problem is the
    'internet' codec in yelp-main/util/internet_encoding.py.
    Once registered, it can be used with as such:
        json.dumps(..., encoding='internet').
    Consider this function fully deprecated as soon as that migrates to yelp_lib

    The only benefit these paired functions have is that strings originally latin1
    will be re-encoded in latin1 when you call loads_with_bytes(). I don't know
    why you would want that though.

    You do NOT need to do this if you expect all of your input to be in UTF-8
    (and its validated)

    Args:
        serializable_dict: a dictionary to json dump.
        default: encoding function to use with json.
    """
    default = default or yelpy_json_encode
    try:
        return json.dumps(serializable_dict, default=default)
    except UnicodeDecodeError:
        pass

    output_dict, latin_key_paths, latin_value_paths = latin1_decode_item(serializable_dict)
    if latin_key_paths or latin_value_paths:
        output_dict[LATIN1_ENCODING_COOKIE] = dict(keys=latin_key_paths, values=latin_value_paths)

    return json.dumps(output_dict, default=default)

def loads_with_bytes(raw_json_string, object_hook=None):
    """
    Invert the work done by dumps_with_bytes as best we can.

    Args:
        raw_json_string: the json dump to decode.
        object_hook: decoding function to use with json.
    """
    object_hook = object_hook or yelpy_json_decode
    encoded_dict = json.loads(raw_json_string, object_hook=object_hook)
    if not LATIN1_ENCODING_COOKIE in encoded_dict: # bail out for common case
        return encoded_dict

    latin1_encoding_info_dict = encoded_dict.pop(LATIN1_ENCODING_COOKIE)

    key_paths = latin1_encoding_info_dict.get('keys')
    value_paths = latin1_encoding_info_dict.get('values')
    if key_paths or value_paths:
        output_dict = latin1_encode_item(encoded_dict, key_paths, value_paths)
    else:
        output_dict = encoded_dict

    return output_dict

def yelpy_json_encode(obj):
    """
    Used as "default" keyword argument for json.dumps. Used to allow json.dumps
    the ability to handle objects in JSONEncoderMetaClass.type_to_encoder_map (examples include
    dates, datetimes, times...) and objects that implement _json_encode.

    Here's an example:

    class MrT:
        def __init__(a, b):
            self.a = a
            self.b = b

        def _json_encode(self):
            return {'a':a, 'b': b}

        @classmethod
        def _json_decode(cls, json_dict):
            return cls(json_dict[a], json_dict[b])

    Our current standard for using one _json_encode and _json_decode versus making a new JSONEncoder class
    is that _json_encode and _json_decode should be used when you can and you only need JSONEncoders for
    built in types or objects from different libraries.
    """

    # Get the full dotted path for the object
    obj_type_str = yelp_lib._import.get_full_dotted_type_string(obj)

    # If this object shows up in our type_to_encoder_map, use the encoder
    # specified in the map
    if obj_type_str in JSONEncoderMetaClass.type_to_encoder_map:
        return {
            '__type__': obj_type_str,
            'json_encoded_object': JSONEncoderMetaClass.type_to_encoder_map[obj_type_str].encode(obj)
        }

    # If the object knows how to encode itself, let it encode itself
    if hasattr(obj, '_json_encode'):
        # Normally the class of the object (obj_type_str) will be used to decode
        # the encoded object. However, if your class is dynamically generated (via a
        # meta class) then obj_type_str might be insufficient. In those cases,
        # your dynamic class should include a member '_json_decoder_class' that
        # is a dotted type string for the class that can decode your dynamic class.
        # See yelp.models.types.NamedValue for an example.
        decoder_class = getattr(obj, '_json_decoder_class', None) or obj_type_str

        return {
            '__type__': decoder_class,
            'json_encoded_object': obj._json_encode()
        }
    elif hasattr(obj, '__primitive__'):
        return obj.__primitive__()

    # Else return the original default function
    raise EncodeError(u"Can't encode: %s %s" % (type(obj), obj))

def yelpy_json_decode(json_dict):
    """
    Used as object_hook keyword argument for json.loads. Used to allow json.loads
    the ability to handle objects in JSONEncoderMetaClass.type_to_encoder_map (examples include
    dates, datetimes, times...) and objects that implement _json_decode.

    Note: This object_hook is called after JSONDecoder.decode() runs so we get dicts
    rather than strings as arguments. Please look at the official python doc for more information
    about json: http://docs.python.org/library/json.html

    Please refer to yelpy_json_encode for an example!
    """
    # If the json dict has a __type__ we will assume we need special parsing for the item
    obj_type_str = json_dict.get('__type__')

    # If this object type shows up in our type_to_encoder_map, use the decoder specified
    # in that map
    if obj_type_str in JSONEncoderMetaClass.type_to_encoder_map:
        return JSONEncoderMetaClass.type_to_encoder_map[obj_type_str].decode(json_dict['json_encoded_object'])

    # If we have an obj_type_str and it's not in the type_to_encoder_map (prebuilt encoders
    # that we already know about), the object should have a _json_decode method
    # if it wasn't encoded using proto_encode. proto_encode will always store
    # the class name in __type__ instead of a full module path, allowing us to
    # differentiate between the two systems for now. This object-hook trickery
    # should be considered deprecated. If you want to build objects from JSON
    # strings, use yelp_logging.proto_encode
    if obj_type_str is not None and "." in obj_type_str:
        obj_class = yelp_lib._import.import_module_class(obj_type_str)

        # If we don't have a _json_decode function, blow up!
        if not hasattr(obj_class, '_json_decode'):
            raise DecodeError("Cant decode: %s" % obj_class)

        return obj_class._json_decode(json_dict['json_encoded_object'])

    # Else return the json_dict. It's probably already parsed at this point
    return json_dict


class JSONEncoderMetaClass(type):
    """
    Metaclass to propogate type_to_encoder_map so we don't have to maintain the encoder classes
    separate from the map of objects they encode
    """

    # Map from the name of the module to the an Encoder class
    # This is used so we can get the right encoder for the type we're trying to decode/encode.
    type_to_encoder_map = {}

    def __new__(cls, name, bases, attrs):

        # Make the new class using our parents __new__ function
        json_encoder_class = super(JSONEncoderMetaClass, cls).__new__(cls, name, bases, attrs)

        # Get the encoded type from the json_encoder_class
        encoded_type = getattr(json_encoder_class, 'encoded_type')

        # If there's an encoded_type, update type_to_encoder_map
        if encoded_type is not None:
            cls.type_to_encoder_map["%s.%s" % (encoded_type.__module__, encoded_type.__name__)] = json_encoder_class

        return json_encoder_class


class AbstractJSONEncoder(object):
    """
    Abstract Encoder class that gets added to the type_to_encoder_map via its metaclass JSONEncoderMetaClass.
    Must set a encode function and a decode function
    """
    __metaclass__ = JSONEncoderMetaClass

    # Must set this. This is the type that the jsonencoder encodes. Used to help JSONEncoderMetaClass
    # propogate type_to_encoder_map
    encoded_type = None

    '''
    @staticmethod
    def encode(encodable_object):
        """
        Encode the object. Should return a json encodable object (ie. a dict of strs and ints)
        """
        raise NotImplementedError

    @staticmethod
    def decode(encoded_object):
        """
        Decode the object using the stuff that comes out of encode
        """
        raise NotImplementedError
    '''

class DatetimeJSONEncoder(AbstractJSONEncoder):
    """
    Encoder for the datetime object
    """
    encoded_type = datetime.datetime

    @staticmethod
    def encode(datetime_object):
        assert datetime_object.tzinfo is None

        # timetuple returns (year, month, day, hour, minute, second). The constructor
        # of datetime also takes a microsecond field
        return list(datetime_object.timetuple()[:6] + (datetime_object.microsecond,))

    @staticmethod
    def decode(encoded_object):
        return datetime.datetime(*encoded_object)

class DateJSONEncoder(AbstractJSONEncoder):
    """
    Encoder for date objects
    """
    encoded_type = datetime.date

    @staticmethod
    def encode(date_object):

        # date takes only needs 3 constructors for its creation (year, month, day)
        return list(date_object.timetuple()[:3])

    @staticmethod
    def decode(encoded_object):
        return datetime.date(*encoded_object)

class TimeJSONEncoder(AbstractJSONEncoder):
    """
    Encoder for time objects
    """
    encoded_type = datetime.time

    @staticmethod
    def encode(time_object):
        assert time_object.tzinfo is None
        return [time_object.hour, time_object.minute, time_object.second, time_object.microsecond]

    @staticmethod
    def decode(encoded_object):
        return datetime.time(*encoded_object)

class DecimalJSONEncoder(AbstractJSONEncoder):
    """
    Encoder for decimal objects
    """
    encoded_type = decimal.Decimal

    @staticmethod
    def encode(decimal_object):
        return unicode(decimal_object)

    @staticmethod
    def decode(encoded_object):
        return decimal.Decimal(encoded_object)

class SetJSONEncoder(AbstractJSONEncoder):
    """
    Encoder for set objects
    """
    encoded_type = set

    @staticmethod
    def encode(a_set):
        return list(a_set)

    @staticmethod
    def decode(encoded_object):
        return set(encoded_object)

# XXX as per #35057, don't re-add the load/dump/loads/dumps names here

# convenient names for JSON decoding with the yelpy_json_(en,de)code hooks
# these are not deprecated but don't use them unless you're sure you need them
# (you probably don't)
dumps_serialized = functools.partial(json.dumps, default=yelpy_json_encode)
dump_serialized = functools.partial(json.dump, default=yelpy_json_encode)
loads_serialized = functools.partial(json.loads, object_hook=yelpy_json_decode)
load_serialized = functools.partial(json.load, object_hook=yelpy_json_decode)

# these are DEPRECATED and should not be used:
def __str_fallback(obj):
    # This is a forward-compatibility shim for a __primitive__ interface.
    # Don't use this directly.
    if hasattr(obj, '__primitive__'):
        return obj.__primitive__()
    else:
        return str(obj)
dumps_with_str = functools.partial(json.dumps, default=__str_fallback)
dump_with_str = functools.partial(json.dump, default=__str_fallback)

end_script_re = re.compile(r'</script.*?>', re.IGNORECASE)

def dumps_safe(value, sanitize=False):
    jstr = json.dumps(value, default=__str_fallback)
    if end_script_re.search(jstr):
        if sanitize:
            while end_script_re.search(jstr):
                jstr = end_script_re.sub('', jstr)
        else:
            raise EncodeError('unsafe content found in JSON output')
    return jstr
